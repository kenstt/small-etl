use crate::config::sequence_config::PipelineDefinition;
use crate::core::{
    pipeline_sequence::{ContextualPipeline, PipelineContext},
    Record, Storage, TransformResult,
};
use crate::utils::error::{EtlError, Result};
use reqwest::Client;
use std::collections::HashMap;
use std::io::Write;
use zip::write::{FileOptions, ZipWriter};

/// åŸºæ–¼åºåˆ—é…ç½®çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥ Pipeline
pub struct SequenceAwarePipeline<S: Storage> {
    name: String,
    storage: S,
    config: PipelineDefinition,
    client: Client,
}

impl<S: Storage> SequenceAwarePipeline<S> {
    pub fn new(name: String, storage: S, config: PipelineDefinition) -> Self {
        Self {
            name,
            storage,
            config,
            client: Client::new(),
        }
    }

    /// æ±ºå®šæ•¸æ“šä¾†æºï¼šAPIã€å‰ä¸€å€‹ Pipeline æˆ–åˆä½µ
    async fn determine_data_source(&self, context: &PipelineContext) -> Result<Vec<Record>> {
        let mut records = Vec::new();

        // æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å‰ä¸€å€‹ Pipeline çš„è¼¸å‡º
        if let Some(data_source) = &self.config.source.data_source {
            if data_source.use_previous_output.unwrap_or(false) {
                if let Some(from_pipeline) = &data_source.from_pipeline {
                    // ä½¿ç”¨æŒ‡å®š Pipeline çš„è¼¸å‡º
                    if let Some(pipeline_result) = context.get_result_by_name(from_pipeline) {
                        records.extend(pipeline_result.records.clone());
                        tracing::info!(
                            "ğŸ“‚ {}: Using {} records from pipeline '{}'",
                            self.name,
                            records.len(),
                            from_pipeline
                        );
                    }
                } else {
                    // ä½¿ç”¨å‰ä¸€å€‹ Pipeline çš„è¼¸å‡º
                    if let Some(previous_result) = context.get_previous_result() {
                        records.extend(previous_result.records.clone());
                        tracing::info!(
                            "ğŸ“‚ {}: Using {} records from previous pipeline",
                            self.name,
                            records.len()
                        );
                    }
                }

                // å¦‚æœè¨­å®šç‚ºåˆä½µï¼Œé‚„éœ€è¦ç²å– API æ•¸æ“š
                // ä½†å°æ–¼åƒæ•¸åŒ– APIï¼ˆå« {param}ï¼‰ï¼Œå³ä½¿ merge_with_api = false ä¹Ÿéœ€è¦åŸ·è¡Œ API å‘¼å«
                let endpoint = self.config.source.endpoint.as_deref().unwrap_or("");
                if !data_source.merge_with_api.unwrap_or(false) && !endpoint.contains("{") {
                    return Ok(records);
                }
            }
        }

        // ç²å– API æ•¸æ“š - æª¢æŸ¥æ˜¯å¦éœ€è¦åƒæ•¸åŒ–å‘¼å«
        let endpoint = self.config.source.endpoint.as_deref().unwrap_or("");

        // å°æ–¼ "previous" å’Œ "combined" é¡å‹ï¼Œä¸é€²è¡Œ API å‘¼å«
        if self.config.source.r#type == "previous" || self.config.source.r#type == "combined" {
            return Ok(records);
        }

        // å¦‚æœæ²’æœ‰ç«¯é»ï¼Œä¹Ÿä¸é€²è¡Œ API å‘¼å«
        if endpoint.is_empty() {
            return Ok(records);
        }

        let api_records = if endpoint.contains("{") {
            // åƒæ•¸åŒ– API å‘¼å« - æ›¿æ›å‰ä¸€å€‹ pipeline çš„æ•¸æ“š
            return self.fetch_parameterized_api(context).await;
        } else {
            // æ¨™æº– API å‘¼å«
            self.fetch_api_data().await?
        };
        records.extend(api_records);

        Ok(records)
    }

    /// è™•ç†åƒæ•¸åŒ– API å‘¼å«ï¼ˆç‚ºæ¯å€‹å‰ä¸€å€‹è¨˜éŒ„åˆ†åˆ¥å‘¼å«ï¼‰
    async fn fetch_parameterized_api(&self, context: &PipelineContext) -> Result<Vec<Record>> {
        let mut all_records = Vec::new();

        // ç²å–å‰ä¸€å€‹ Pipeline çš„è¨˜éŒ„ä½œç‚ºåƒæ•¸æº
        let param_records = if let Some(data_source) = &self.config.source.data_source {
            if data_source.use_previous_output.unwrap_or(false) {
                if let Some(from_pipeline) = &data_source.from_pipeline {
                    context.get_result_by_name(from_pipeline)
                        .map(|r| r.records.clone())
                        .unwrap_or_default()
                } else {
                    context.get_previous_result()
                        .map(|r| r.records.clone())
                        .unwrap_or_default()
                }
            } else {
                Vec::new()
            }
        } else {
            Vec::new()
        };

        tracing::info!("ğŸ“¡ {}: Making parameterized API calls for {} records", self.name, param_records.len());

        // ç‚ºæ¯å€‹è¨˜éŒ„æ§‹å»ºä¸¦å‘¼å« API
        for (index, record) in param_records.iter().enumerate() {
            let endpoint = self.build_parameterized_endpoint(&record.data)?;
            tracing::debug!("ğŸ“¡ {}: API call {}/{}: {}", self.name, index + 1, param_records.len(), endpoint);

            let api_records = self.fetch_single_api_call_with_data(&endpoint, Some(&record.data)).await?;
            all_records.extend(api_records);

            // å¯é¸ï¼šæ·»åŠ å»¶é²é¿å…è«‹æ±‚éæ–¼é »ç¹
            if index < param_records.len() - 1 {
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }
        }

        tracing::info!("ğŸ“¡ {}: Total records fetched from parameterized APIs: {}", self.name, all_records.len());
        Ok(all_records)
    }

    /// è™•ç† payload æ¨¡æ¿ï¼Œæ›¿æ›åƒæ•¸
    fn process_payload_template(&self, template: &str, record_data: Option<&HashMap<String, serde_json::Value>>) -> Result<String> {
        let mut processed = template.to_string();

        // å¦‚æœé…ç½®äº†æ¨¡æ¿åƒæ•¸æ˜ å°„
        if let Some(payload_config) = &self.config.source.payload {
            if let Some(template_params) = &payload_config.template_params {
                for (template_key, data_key) in template_params {
                    let placeholder = format!("{{{{{}}}}}", template_key);
                    if processed.contains(&placeholder) {
                        if let Some(record_data) = record_data {
                            if let Some(value) = record_data.get(data_key) {
                                let value_str = match value {
                                    serde_json::Value::String(s) => s.clone(),
                                    serde_json::Value::Number(n) => n.to_string(),
                                    serde_json::Value::Bool(b) => b.to_string(),
                                    serde_json::Value::Null => "null".to_string(),
                                    _ => serde_json::to_string(value).unwrap_or_default().trim_matches('"').to_string(),
                                };
                                processed = processed.replace(&placeholder, &value_str);
                                tracing::debug!("ğŸ“¡ {}: Replaced payload template {} with {}", self.name, placeholder, value_str);
                            }
                        }
                    }
                }
            }

            // å¦‚æœå•Ÿç”¨äº†ä½¿ç”¨å‰ä¸€å€‹ pipeline è³‡æ–™ä½œç‚ºåƒæ•¸
            if payload_config.use_previous_data_as_params.unwrap_or(false) {
                if let Some(record_data) = record_data {
                    for (key, value) in record_data {
                        let placeholder = format!("{{{{{}}}}}", key);
                        if processed.contains(&placeholder) {
                            let value_str = match value {
                                serde_json::Value::String(s) => s.clone(),
                                serde_json::Value::Number(n) => n.to_string(),
                                serde_json::Value::Bool(b) => b.to_string(),
                                serde_json::Value::Null => "null".to_string(),
                                _ => serde_json::to_string(value).unwrap_or_default().trim_matches('"').to_string(),
                            };
                            processed = processed.replace(&placeholder, &value_str);
                            tracing::debug!("ğŸ“¡ {}: Replaced payload parameter {} with {}", self.name, placeholder, value_str);
                        }
                    }
                }
            }
        }

        // æª¢æŸ¥æ˜¯å¦é‚„æœ‰æœªæ›¿æ›çš„åƒæ•¸
        if processed.contains("{{") && processed.contains("}}") {
            tracing::warn!("ğŸ“¡ {}: Unresolved template parameters in payload: {}", self.name, processed);
        }

        Ok(processed)
    }

    /// æ§‹å»ºåƒæ•¸åŒ–ç«¯é» URL
    fn build_parameterized_endpoint(&self, data: &HashMap<String, serde_json::Value>) -> Result<String> {
        let mut endpoint = self.config.source.endpoint.as_ref()
            .ok_or_else(|| EtlError::ConfigValidationError {
                field: "source.endpoint".to_string(),
                message: "Endpoint is required for parameterized API calls".to_string(),
            })?.clone();

        tracing::debug!("ğŸ“¡ {}: Building endpoint from template: {}", self.name, endpoint);
        tracing::debug!("ğŸ“¡ {}: Available data fields: {:?}", self.name, data.keys().collect::<Vec<_>>());

        // æ›¿æ› URL ä¸­çš„åƒæ•¸ä½”ä½ç¬¦ (æ”¯æ´ {key} å’Œ {{key}} æ ¼å¼)
        for (key, value) in data {
            let placeholder_single = format!("{{{}}}", key);
            let placeholder_double = format!("{{{{{}}}}}", key);

            let value_str = match value {
                serde_json::Value::String(s) => s.clone(),
                serde_json::Value::Number(n) => n.to_string(),
                _ => value.to_string().trim_matches('"').to_string(),
            };

            if endpoint.contains(&placeholder_single) {
                endpoint = endpoint.replace(&placeholder_single, &value_str);
                tracing::info!("ğŸ“¡ {}: Replaced {} with {}", self.name, placeholder_single, value_str);
            } else if endpoint.contains(&placeholder_double) {
                endpoint = endpoint.replace(&placeholder_double, &value_str);
                tracing::info!("ğŸ“¡ {}: Replaced {} with {}", self.name, placeholder_double, value_str);
            }
        }

        tracing::debug!("ğŸ“¡ {}: Final endpoint: {}", self.name, endpoint);

        // æª¢æŸ¥æ˜¯å¦é‚„æœ‰æœªæ›¿æ›çš„åƒæ•¸ (å–®æ‹¬è™Ÿæ ¼å¼)
        if endpoint.contains("{") && endpoint.contains("}") {
            // æ‰¾å‡ºæ‰€æœ‰æœªæ›¿æ›çš„åƒæ•¸
            let re = regex::Regex::new(r"\{([^}]+)\}").unwrap();
            let unresolved: Vec<&str> = re.captures_iter(&endpoint)
                .map(|cap| cap.get(1).unwrap().as_str())
                .collect();

            tracing::error!("ğŸ“¡ {}: Unresolved parameters in endpoint: {}", self.name, endpoint);
            tracing::error!("ğŸ“¡ {}: Unresolved parameter names: {:?}", self.name, unresolved);
            tracing::error!("ğŸ“¡ {}: Available fields were: {:?}", self.name, data.keys().collect::<Vec<_>>());
            return Err(crate::utils::error::EtlError::ProcessingError {
                message: format!("Unresolved parameters in endpoint: {}. Unresolved: {:?}, Available fields: {:?}",
                    endpoint, unresolved, data.keys().collect::<Vec<_>>())
            });
        }

        Ok(endpoint)
    }


    /// åŸ·è¡Œå–®ä¸€ API å‘¼å«ï¼Œæ”¯æ´è³‡æ–™åƒæ•¸
    async fn fetch_single_api_call_with_data(&self, endpoint: &str, record_data: Option<&HashMap<String, serde_json::Value>>) -> Result<Vec<Record>> {
        let mut records = Vec::new();

        // æ±ºå®š HTTP æ–¹æ³•
        let method = self.config.source.method.as_deref().unwrap_or("GET").to_uppercase();

        // æ§‹å»ºè«‹æ±‚
        let mut request = match method.as_str() {
            "GET" => self.client.get(endpoint),
            "POST" => self.client.post(endpoint),
            "PUT" => self.client.put(endpoint),
            "DELETE" => self.client.delete(endpoint),
            "PATCH" => self.client.patch(endpoint),
            "HEAD" => self.client.head(endpoint),
            _ => {
                tracing::warn!("ğŸ“¡ {}: Unsupported HTTP method '{}', falling back to GET", self.name, method);
                self.client.get(endpoint)
            }
        };

        // æ·»åŠ è‡ªå®šç¾©æ¨™é ­
        if let Some(headers) = &self.config.source.headers {
            for (key, value) in headers {
                request = request.header(key, value);
            }
        }

        // è™•ç† payload
        if let Some(payload_config) = &self.config.source.payload {
            // è¨­å®š Content-Type
            if let Some(content_type) = &payload_config.content_type {
                request = request.header("Content-Type", content_type);
            } else if method != "GET" && method != "HEAD" {
                request = request.header("Content-Type", "application/json");
            }

            // è™•ç†è«‹æ±‚é«”
            if let Some(body_template) = &payload_config.body {
                let processed_body = self.process_payload_template(body_template, record_data)?;
                if !processed_body.is_empty() {
                    tracing::debug!("ğŸ“¡ {}: Request body: {}", self.name, processed_body);
                    request = request.body(processed_body);
                }
            }
        }

        // æ·»åŠ æŸ¥è©¢åƒæ•¸
        if let Some(params) = &self.config.source.parameters {
            for (key, value) in params {
                request = request.query(&[(key, value)]);
            }
        }

        // è¨­å®šè¶…æ™‚
        if let Some(timeout) = self.config.source.timeout_seconds {
            request = request.timeout(std::time::Duration::from_secs(timeout));
        }

        tracing::debug!("ğŸ“¡ {}: Making {} request to: {}", self.name, method, endpoint);

        // åŸ·è¡Œè«‹æ±‚
        let response = request.send().await?;

        if response.status().is_success() {
            let json_data: serde_json::Value = response.json().await?;

            // è™•ç† API å›æ‡‰ï¼ˆæ”¯æŒå–®ä¸€ç‰©ä»¶å›æ‡‰ï¼‰
            if let serde_json::Value::Object(obj) = json_data {
                let mut data = HashMap::new();

                // æ‡‰ç”¨å­—æ®µæ˜ å°„
                if let Some(field_mapping) = &self.config.extract.field_mapping {
                    for (original_key, value) in obj {
                        let mapped_key = field_mapping
                            .get(&original_key)
                            .unwrap_or(&original_key);
                        data.insert(mapped_key.clone(), value);
                    }
                } else {
                    // æ²’æœ‰æ˜ å°„å°±ç›´æ¥ä½¿ç”¨åŸå§‹å­—æ®µ
                    for (key, value) in obj {
                        data.insert(key, value);
                    }
                }

                records.push(Record { data });
            } else if let serde_json::Value::Array(items) = json_data {
                // è™•ç†é™£åˆ—å›æ‡‰
                for item in items {
                    if let serde_json::Value::Object(obj) = item {
                        let mut data = HashMap::new();

                        if let Some(field_mapping) = &self.config.extract.field_mapping {
                            for (original_key, value) in obj {
                                let mapped_key = field_mapping
                                    .get(&original_key)
                                    .unwrap_or(&original_key);
                                data.insert(mapped_key.clone(), value);
                            }
                        } else {
                            for (key, value) in obj {
                                data.insert(key, value);
                            }
                        }

                        records.push(Record { data });
                    }
                }
            }
        } else {
            let error_msg = format!("API request failed with status: {}", response.status());
            return Err(crate::utils::error::EtlError::ProcessingError {
                message: error_msg
            });
        }

        Ok(records)
    }

    /// å¾ API ç²å–æ•¸æ“š
    async fn fetch_api_data(&self) -> Result<Vec<Record>> {
        let endpoint = self.config.source.endpoint.as_ref()
            .ok_or_else(|| EtlError::ConfigValidationError {
                field: "source.endpoint".to_string(),
                message: "Endpoint is required for API calls".to_string(),
            })?;

        self.fetch_single_api_call_with_data(endpoint, None).await
    }


    /// æ‡‰ç”¨æ•¸æ“šè™•ç†æ“ä½œ
    fn apply_data_processing(&self, mut records: Vec<Record>) -> Vec<Record> {
        if let Some(processing) = &self.config.extract.data_processing {
            // å»é‡
            if processing.deduplicate.unwrap_or(false) {
                let original_count = records.len();
                if let Some(dedup_fields) = &processing.deduplicate_fields {
                    // åŸºæ–¼æŒ‡å®šå­—æ®µå»é‡
                    let mut seen = std::collections::HashSet::new();
                    records.retain(|record| {
                        let key: Vec<String> = dedup_fields
                            .iter()
                            .map(|field| {
                                record.data.get(field)
                                    .map(|v| v.to_string())
                                    .unwrap_or_default()
                            })
                            .collect();
                        seen.insert(key)
                    });
                } else {
                    // åŸºæ–¼æ•´å€‹è¨˜éŒ„å»é‡
                    let mut seen = std::collections::HashSet::new();
                    records.retain(|record| {
                        let key = serde_json::to_string(&record.data).unwrap_or_default();
                        seen.insert(key)
                    });
                }
                tracing::info!(
                    "ğŸ”„ {}: Deduplicated {} -> {} records",
                    self.name,
                    original_count,
                    records.len()
                );
            }

            // æ’åº
            if let Some(sort_field) = &processing.sort_by {
                let ascending = processing.sort_order.as_deref() != Some("desc");
                records.sort_by(|a, b| {
                    let a_val = a.data.get(sort_field);
                    let b_val = b.data.get(sort_field);

                    let comparison = match (a_val, b_val) {
                        (Some(a), Some(b)) => a.to_string().cmp(&b.to_string()),
                        (Some(_), None) => std::cmp::Ordering::Less,
                        (None, Some(_)) => std::cmp::Ordering::Greater,
                        (None, None) => std::cmp::Ordering::Equal,
                    };

                    if ascending { comparison } else { comparison.reverse() }
                });
                tracing::info!("ğŸ”„ {}: Sorted {} records by '{}'", self.name, records.len(), sort_field);
            }
        }

        records
    }
}

#[async_trait::async_trait]
impl<S: Storage> ContextualPipeline for SequenceAwarePipeline<S> {
    fn get_name(&self) -> &str {
        &self.name
    }

    async fn extract_with_context(&self, context: &PipelineContext) -> Result<Vec<Record>> {
        tracing::info!("ğŸ“¥ {}: Starting contextual extract", self.name);

        // æ±ºå®šæ•¸æ“šä¾†æºä¸¦ç²å–åŸå§‹æ•¸æ“š
        let raw_records = self.determine_data_source(context).await?;

        // æ‡‰ç”¨æ•¸æ“šè™•ç†æ“ä½œ
        let processed_records = self.apply_data_processing(raw_records);

        tracing::info!("ğŸ“¥ {}: Extracted {} records", self.name, processed_records.len());
        Ok(processed_records)
    }

    async fn transform_with_context(
        &self,
        data: Vec<Record>,
        context: &PipelineContext,
    ) -> Result<TransformResult> {
        let mut processed_records = Vec::new();
        let mut csv_lines = vec!["id,data,pipeline,processed".to_string()];
        let mut tsv_lines = vec!["id\tdata\tpipeline\tprocessed".to_string()];
        let mut intermediate_data = Vec::new();

        tracing::info!("ğŸ”„ {}: Starting contextual transform for {} records", self.name, data.len());

        for (index, mut record) in data.into_iter().enumerate() {
            // æ‡‰ç”¨è½‰æ›æ“ä½œ
            if let Some(operations) = &self.config.transform.operations {
                // æ–‡æœ¬æ¸…ç†
                if operations.clean_text.unwrap_or(false) {
                    for (_, value) in record.data.iter_mut() {
                        if let serde_json::Value::String(s) = value {
                            *s = s.trim().replace('\n', " ");
                        }
                    }
                }

                // æ¨™æº–åŒ–å­—æ®µ
                if let Some(normalize_fields) = &operations.normalize_fields {
                    for field in normalize_fields {
                        if let Some(value) = record.data.get_mut(field) {
                            if let serde_json::Value::String(s) = value {
                                *s = s.to_lowercase();
                            }
                        }
                    }
                }
            }

            // æ•¸æ“šè±å¯ŒåŒ–
            if let Some(enrichment) = &self.config.transform.data_enrichment {
                // æŸ¥æ‰¾æ•¸æ“š
                if let Some(lookup_data) = &enrichment.lookup_data {
                    for (lookup_field, target_field) in lookup_data {
                        if let Some(lookup_value) = record.data.get(lookup_field) {
                            // é€™è£¡å¯ä»¥å¯¦ä½œæ›´è¤‡é›œçš„æŸ¥æ‰¾é‚è¼¯
                            record.data.insert(
                                target_field.clone(),
                                serde_json::Value::String(format!("enriched_{}", lookup_value))
                            );
                        }
                    }
                }

                // è¨ˆç®—å­—æ®µ
                if let Some(computed_fields) = &enrichment.computed_fields {
                    for (field_name, expression) in computed_fields {
                        // ç°¡å–®çš„è¨ˆç®—é‚è¼¯ç¤ºä¾‹
                        let computed_value = match expression.as_str() {
                            "record_index" => serde_json::Value::Number(index.into()),
                            "pipeline_name" => serde_json::Value::String(self.name.clone()),
                            "execution_id" => serde_json::Value::String(context.execution_id.clone()),
                            _ => serde_json::Value::String(expression.clone()),
                        };
                        record.data.insert(field_name.clone(), computed_value);
                    }
                }
            }

            // æ·»åŠ è™•ç†æ¨™è¨˜
            record.data.insert("processed".to_string(), serde_json::Value::Bool(true));
            record.data.insert("processed_by".to_string(), serde_json::Value::String(self.name.clone()));

            // ç”Ÿæˆè¼¸å‡ºæ ¼å¼
            let id = record.data.get("id")
                .and_then(|v| v.as_i64())
                .unwrap_or(index as i64);
            let data_summary = format!("record_{}", index);

            csv_lines.push(format!("{},{},{},true", id, data_summary, self.name));
            tsv_lines.push(format!("{}\t{}\t{}\ttrue", id, data_summary, self.name));

            // æª¢æŸ¥ä¸­ç¹¼æ•¸æ“šæ¢ä»¶
            if let Some(intermediate_config) = &self.config.transform.intermediate {
                let mut meets_conditions = true;

                if let Some(conditions) = &intermediate_config.conditions {
                    for (field, expected_value) in conditions {
                        if let Some(actual_value) = record.data.get(field) {
                            if actual_value != expected_value {
                                meets_conditions = false;
                                break;
                            }
                        } else {
                            meets_conditions = false;
                            break;
                        }
                    }
                }

                if meets_conditions {
                    intermediate_data.push(record.clone());

                    // å°å‡ºåˆ°å…±äº«æ•¸æ“š
                    if intermediate_config.export_to_shared.unwrap_or(false) {
                        if let Some(shared_key) = &intermediate_config.shared_key {
                            // é€™è£¡éœ€è¦ä¿®æ”¹ context ä¾†æ›´æ–°å…±äº«æ•¸æ“š
                            // ç›®å‰çš„å¯¦ä½œä¸­ context æ˜¯ä¸å¯è®Šçš„ï¼Œéœ€è¦é‡æ–°è¨­è¨ˆ
                            tracing::debug!("ğŸ“¤ {}: Would export to shared data with key '{}'", self.name, shared_key);
                        }
                    }
                }
            }

            processed_records.push(record);
        }

        tracing::info!(
            "ğŸ”„ {}: Transform complete: {} processed, {} intermediate",
            self.name,
            processed_records.len(),
            intermediate_data.len()
        );

        Ok(TransformResult {
            processed_records,
            csv_output: csv_lines.join("\n"),
            tsv_output: tsv_lines.join("\n"),
            intermediate_data,
        })
    }

    async fn load_with_context(
        &self,
        result: TransformResult,
        context: &PipelineContext,
    ) -> Result<String> {
        let filename = if let Some(pattern) = &self.config.load.filename_pattern {
            // ç°¡å–®çš„æ¨¡æ¿æ›¿æ›
            pattern
                .replace("{pipeline_name}", &self.name)
                .replace("{execution_id}", &context.execution_id)
                .replace("{timestamp}", &chrono::Utc::now().format("%Y%m%d_%H%M%S").to_string())
        } else {
            format!("{}_output.zip", self.name)
        };

        let output_path = format!("{}/{}", self.config.load.output_path, filename);

        tracing::info!("ğŸ’¾ {}: Starting contextual load to: {}", self.name, output_path);

        // å‰µå»º ZIP æ–‡ä»¶
        let zip_data = {
            let mut zip = ZipWriter::new(std::io::Cursor::new(Vec::new()));

            // æ ¹æ“šé…ç½®çš„è¼¸å‡ºæ ¼å¼æ·»åŠ æ–‡ä»¶
            for format in &self.config.load.output_formats {
                match format.as_str() {
                    "csv" => {
                        zip.start_file::<_, ()>("output.csv", FileOptions::default())?;
                        zip.write_all(result.csv_output.as_bytes())?;
                    }
                    "tsv" => {
                        zip.start_file::<_, ()>("output.tsv", FileOptions::default())?;
                        zip.write_all(result.tsv_output.as_bytes())?;
                    }
                    "json" => {
                        zip.start_file::<_, ()>("processed_data.json", FileOptions::default())?;
                        let json_data = serde_json::to_string_pretty(&result.processed_records)?;
                        zip.write_all(json_data.as_bytes())?;
                    }
                    _ => {
                        tracing::warn!("ğŸ”¶ {}: Unsupported output format: {}", self.name, format);
                    }
                }
            }

            // æ·»åŠ ä¸­ç¹¼çµæœ JSON
            if !result.intermediate_data.is_empty() {
                zip.start_file::<_, ()>("intermediate.json", FileOptions::default())?;
                let json_data = serde_json::to_string_pretty(&result.intermediate_data)?;
                zip.write_all(json_data.as_bytes())?;
            }

            // æ·»åŠ å…ƒæ•¸æ“š
            if let Some(compression) = &self.config.load.compression {
                if compression.include_metadata.unwrap_or(false) {
                    zip.start_file::<_, ()>("metadata.json", FileOptions::default())?;
                    let mut metadata = HashMap::new();
                    metadata.insert("pipeline_name".to_string(), serde_json::Value::String(self.name.clone()));
                    metadata.insert("execution_id".to_string(), serde_json::Value::String(context.execution_id.clone()));
                    metadata.insert("timestamp".to_string(), serde_json::Value::String(chrono::Utc::now().to_rfc3339()));
                    let metadata_json = serde_json::to_string_pretty(&metadata)?;
                    zip.write_all(metadata_json.as_bytes())?;
                }
            }

            // å®Œæˆä¸¦å–å›åº•å±¤ Vec<u8>
            let cursor = zip.finish()?;
            cursor.into_inner()
        };

        // ä¿å­˜ ZIP æ–‡ä»¶
        self.storage.write_file(&filename, &zip_data).await?;

        tracing::info!("ğŸ’¾ {}: Load completed successfully", self.name);
        Ok(output_path)
    }

    fn should_execute(&self, context: &PipelineContext) -> bool {
        // æª¢æŸ¥æ˜¯å¦å•Ÿç”¨
        if !self.config.enabled.unwrap_or(true) {
            return false;
        }

        // æª¢æŸ¥åŸ·è¡Œæ¢ä»¶
        if let Some(conditions) = &self.config.conditions {
            // æª¢æŸ¥å‰ä¸€å€‹ Pipeline æ˜¯å¦æˆåŠŸ
            if let Some(when_previous_succeeded) = conditions.when_previous_succeeded {
                if when_previous_succeeded && context.get_previous_result().is_none() {
                    return false;
                }
            }

            // æª¢æŸ¥è¨˜éŒ„æ•¸æ¢ä»¶
            if let Some(record_condition) = &conditions.when_records_count {
                let record_count = if let Some(from_pipeline) = &record_condition.from_pipeline {
                    context.get_result_by_name(from_pipeline)
                        .map(|r| r.records.len())
                        .unwrap_or(0)
                } else {
                    context.get_previous_result()
                        .map(|r| r.records.len())
                        .unwrap_or(0)
                };

                if let Some(min) = record_condition.min {
                    if record_count < min {
                        return false;
                    }
                }

                if let Some(max) = record_condition.max {
                    if record_count > max {
                        return false;
                    }
                }
            }

            // æª¢æŸ¥å…±äº«æ•¸æ“šæ¢ä»¶
            if let Some(shared_conditions) = &conditions.when_shared_data {
                for (key, expected_value) in shared_conditions {
                    if let Some(actual_value) = context.get_shared_data(key) {
                        if actual_value != expected_value {
                            return false;
                        }
                    } else {
                        return false;
                    }
                }
            }
        }

        true
    }

}