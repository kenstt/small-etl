use clap::Parser;
use samll_etl::config::sequence_config::SequenceConfig;
use samll_etl::core::{
    contextual_pipeline::SequenceAwarePipeline, pipeline_sequence::PipelineSequence,
};
use samll_etl::utils::logger;
use samll_etl::LocalStorage;
use std::collections::HashMap;

#[derive(Parser)]
#[command(name = "sequence-etl")]
#[command(about = "ETL tool with pipeline sequence support")]
struct Args {
    /// Path to sequence configuration file
    #[arg(short, long, default_value = "configs/sequence-example.toml")]
    config: String,

    /// Enable verbose output
    #[arg(short, long)]
    verbose: bool,

    /// Override monitoring setting from config
    #[arg(long)]
    monitor: Option<bool>,

    /// Dry run - show execution plan without executing
    #[arg(long)]
    dry_run: bool,

    /// Execution ID for this run
    #[arg(long)]
    execution_id: Option<String>,

    /// Execute only specific pipelines (comma-separated)
    #[arg(long)]
    only: Option<String>,

    /// Skip specific pipelines (comma-separated)
    #[arg(long)]
    skip: Option<String>,
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();

    // åˆå§‹åŒ–æ—¥èªŒ
    logger::init_cli_logger(args.verbose);

    tracing::info!("ğŸš€ Starting Pipeline Sequence ETL tool");
    tracing::info!("ğŸ“ Loading sequence configuration from: {}", args.config);

    // è¼‰å…¥åºåˆ—é…ç½®
    let config = match SequenceConfig::from_file(&args.config) {
        Ok(config) => config,
        Err(e) => {
            eprintln!(
                "âŒ Failed to load sequence config file '{}': {}",
                args.config, e
            );
            eprintln!("ğŸ’¡ Make sure the file exists and is valid TOML format");
            std::process::exit(1);
        }
    };

    // é©—è­‰é…ç½®
    if let Err(e) = config.validate() {
        tracing::error!("âŒ Sequence configuration validation failed: {}", e);
        eprintln!("âŒ {}", e);
        std::process::exit(1);
    }

    tracing::info!("âœ… Sequence configuration loaded and validated successfully");

    // ç”ŸæˆåŸ·è¡Œ ID
    let execution_id = args
        .execution_id
        .clone()
        .unwrap_or_else(|| format!("seq_{}", chrono::Utc::now().format("%Y%m%d_%H%M%S")));

    // é¡¯ç¤ºåºåˆ—æ‘˜è¦
    display_sequence_summary(&config, &args, &execution_id);

    if args.dry_run {
        tracing::info!("ğŸ” DRY RUN MODE - No actual processing will occur");
        perform_dry_run(&config, &args).await?;
        return Ok(());
    }

    // æ±ºå®šç›£æ§è¨­å®š
    let monitor_enabled = args.monitor.unwrap_or_else(|| {
        config
            .monitoring
            .as_ref()
            .map(|m| m.enabled)
            .unwrap_or(false)
    });

    // å‰µå»ºåºåˆ—åŸ·è¡Œå™¨
    let mut sequence = PipelineSequence::new(execution_id.clone()).with_monitoring(monitor_enabled);

    // ç²å–è¦åŸ·è¡Œçš„ Pipeline åˆ—è¡¨
    let pipelines_to_execute = determine_pipelines_to_execute(&config, &args);

    // ç‚ºæ¯å€‹è¦åŸ·è¡Œçš„ Pipeline å‰µå»º ContextualPipeline
    for pipeline_def in pipelines_to_execute {
        tracing::info!("ğŸ“¦ Setting up pipeline: {}", pipeline_def.name);

        // å‰µå»ºå­˜å„²ï¼ˆæ¯å€‹ Pipeline ä½¿ç”¨ç¨ç«‹çš„å­˜å„²ï¼‰
        let storage = LocalStorage::new(pipeline_def.load.output_path.clone());

        // å‰µå»º SequenceAwarePipeline
        let contextual_pipeline =
            SequenceAwarePipeline::new(pipeline_def.name.clone(), storage, pipeline_def.clone());

        sequence.add_pipeline(Box::new(contextual_pipeline));
    }

    // åŸ·è¡Œåºåˆ—
    tracing::info!("ğŸ¬ Starting pipeline sequence execution");
    match sequence.execute_all().await {
        Ok(results) => {
            tracing::info!("ğŸ‰ Pipeline sequence completed successfully!");

            // é¡¯ç¤ºåŸ·è¡Œçµæœæ‘˜è¦
            display_execution_results(&results, &execution_id);

            // åŒ¯å‡ºåŸ·è¡Œæ‘˜è¦
            if let Some(monitoring) = &config.monitoring {
                if monitoring.export_metrics.unwrap_or(false) {
                    export_execution_metrics(&results, &execution_id, monitoring).await?;
                }
            }

            println!("âœ… Pipeline sequence completed successfully!");
            println!("ğŸ†” Execution ID: {}", execution_id);
            println!("ğŸ“Š Pipelines executed: {}", results.len());
        }
        Err(e) => {
            tracing::error!("âŒ Pipeline sequence failed: {}", e);
            eprintln!("âŒ Pipeline sequence failed: {}", e);

            // æ ¹æ“šéŒ¯èª¤è™•ç†é…ç½®æ±ºå®šè™•ç†æ–¹å¼
            if let Some(error_config) = &config.error_handling {
                match error_config.on_pipeline_failure.as_deref() {
                    Some("continue") => {
                        tracing::info!("âš ï¸ Continuing despite failure (configured behavior)");
                        return Ok(());
                    }
                    Some("retry") => {
                        tracing::info!("ğŸ”„ Retry logic would be implemented here");
                        // é€™è£¡å¯ä»¥å¯¦ä½œé‡è©¦é‚è¼¯
                    }
                    _ => {
                        // é è¨­æ˜¯åœæ­¢
                        std::process::exit(1);
                    }
                }
            }

            std::process::exit(1);
        }
    }

    Ok(())
}

fn display_sequence_summary(config: &SequenceConfig, args: &Args, execution_id: &str) {
    println!("ğŸ“‹ Pipeline Sequence Summary:");
    println!(
        "  Name: {} v{}",
        config.sequence.name, config.sequence.version
    );
    println!("  Description: {}", config.sequence.description);
    println!("  Execution ID: {}", execution_id);
    println!("  Total Pipelines: {}", config.pipelines.len());

    if args.dry_run {
        println!("  ğŸ” DRY RUN MODE ENABLED");
    }

    if let Some(only) = &args.only {
        println!("  ğŸ¯ Only executing: {}", only);
    }

    if let Some(skip) = &args.skip {
        println!("  â­ï¸ Skipping: {}", skip);
    }

    println!();
    println!("ğŸ“ Execution Order:");
    for (index, pipeline_name) in config.sequence.execution_order.iter().enumerate() {
        if let Some(pipeline) = config.get_pipeline(pipeline_name) {
            let status = if pipeline.enabled.unwrap_or(true) {
                "âœ…"
            } else {
                "â¸ï¸"
            };
            println!(
                "  {}. {} {} - {}",
                index + 1,
                status,
                pipeline_name,
                pipeline.description.as_deref().unwrap_or("No description")
            );

            if let Some(deps) = &pipeline.dependencies {
                println!("     Dependencies: {}", deps.join(", "));
            }
        }
    }
    println!();
}

fn determine_pipelines_to_execute<'a>(
    config: &'a SequenceConfig,
    args: &'a Args,
) -> Vec<&'a samll_etl::config::sequence_config::PipelineDefinition> {
    let mut pipelines = config.get_enabled_pipelines();

    // è™•ç† --only åƒæ•¸
    if let Some(only_list) = &args.only {
        let only_names: std::collections::HashSet<&str> =
            only_list.split(',').map(|s| s.trim()).collect();
        pipelines.retain(|p| only_names.contains(p.name.as_str()));
    }

    // è™•ç† --skip åƒæ•¸
    if let Some(skip_list) = &args.skip {
        let skip_names: std::collections::HashSet<&str> =
            skip_list.split(',').map(|s| s.trim()).collect();
        pipelines.retain(|p| !skip_names.contains(p.name.as_str()));
    }

    pipelines
}

async fn perform_dry_run(
    config: &SequenceConfig,
    args: &Args,
) -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” Dry Run Analysis:");
    println!();

    let pipelines_to_execute = determine_pipelines_to_execute(config, args);

    for (index, pipeline) in pipelines_to_execute.iter().enumerate() {
        println!("ğŸ“¦ Pipeline {}: {}", index + 1, pipeline.name);
        println!(
            "  ğŸ“¡ Source: {}",
            pipeline
                .source
                .endpoint
                .as_deref()
                .unwrap_or("Previous pipeline output")
        );

        if let Some(data_source) = &pipeline.source.data_source {
            if data_source.use_previous_output.unwrap_or(false) {
                println!("  ğŸ“‚ Uses previous pipeline output");
                if let Some(from_pipeline) = &data_source.from_pipeline {
                    println!("  ğŸ“‚ Specifically from: {}", from_pipeline);
                }
                if data_source.merge_with_api.unwrap_or(false) {
                    println!("  ğŸ”„ Merges with API data");
                }
            }
        }

        if let Some(max_records) = pipeline.extract.max_records {
            println!("  ğŸ“Š Max records: {}", max_records);
        }

        println!("  ğŸ’¾ Output: {}", pipeline.load.output_path);
        println!("  ğŸ“„ Formats: {}", pipeline.load.output_formats.join(", "));

        if let Some(conditions) = &pipeline.conditions {
            println!("  âš™ï¸ Execution conditions:");
            if let Some(when_prev) = conditions.when_previous_succeeded {
                println!("    - Requires previous success: {}", when_prev);
            }
            if conditions.skip_if_empty.unwrap_or(false) {
                println!("    - Skip if no data");
            }
        }

        if let Some(deps) = &pipeline.dependencies {
            println!("  ğŸ”— Dependencies: {}", deps.join(", "));
        }

        println!();
    }

    println!("ğŸ“Š Summary:");
    println!(
        "  Total pipelines to execute: {}",
        pipelines_to_execute.len()
    );
    println!("  Estimated total time: Variable (depends on data size and API response time)");
    println!();
    println!("âœ… Dry run analysis complete.");

    Ok(())
}

fn display_execution_results(
    results: &[samll_etl::core::pipeline_sequence::PipelineResult],
    execution_id: &str,
) {
    println!();
    println!("ğŸ“Š Execution Results Summary:");
    println!("  Execution ID: {}", execution_id);
    println!("  Completed Pipelines: {}", results.len());

    let total_records: usize = results.iter().map(|r| r.records.len()).sum();
    let total_duration: std::time::Duration = results.iter().map(|r| r.duration).sum();

    println!("  Total Records Processed: {}", total_records);
    println!("  Total Execution Time: {:?}", total_duration);
    println!();

    println!("ğŸ“ Pipeline Details:");
    for (index, result) in results.iter().enumerate() {
        println!(
            "  {}. {} - {} records in {:?}",
            index + 1,
            result.pipeline_name,
            result.records.len(),
            result.duration
        );
        println!("     Output: {}", result.output_path);
    }
    println!();
}

async fn export_execution_metrics(
    results: &[samll_etl::core::pipeline_sequence::PipelineResult],
    execution_id: &str,
    monitoring_config: &samll_etl::config::sequence_config::MonitoringConfig,
) -> Result<(), Box<dyn std::error::Error>> {
    let metrics_file = monitoring_config
        .metrics_file
        .as_deref()
        .unwrap_or("sequence_metrics.json");

    let mut metrics = HashMap::new();
    metrics.insert(
        "execution_id",
        serde_json::Value::String(execution_id.to_string()),
    );
    metrics.insert(
        "timestamp",
        serde_json::Value::String(chrono::Utc::now().to_rfc3339()),
    );

    let summary = PipelineSequence::get_execution_summary(results);
    metrics.insert(
        "summary",
        serde_json::Value::Object(summary.into_iter().collect()),
    );

    let pipeline_metrics: Vec<serde_json::Value> = results
        .iter()
        .map(|result| {
            let mut pipeline_data = HashMap::new();
            pipeline_data.insert(
                "name".to_string(),
                serde_json::Value::String(result.pipeline_name.clone()),
            );
            pipeline_data.insert(
                "records_count".to_string(),
                serde_json::Value::Number(result.records.len().into()),
            );
            pipeline_data.insert(
                "duration_ms".to_string(),
                serde_json::Value::Number((result.duration.as_millis() as u64).into()),
            );
            pipeline_data.insert(
                "output_path".to_string(),
                serde_json::Value::String(result.output_path.clone()),
            );

            for (key, value) in &result.metadata {
                pipeline_data.insert(key.clone(), value.clone());
            }

            serde_json::Value::Object(pipeline_data.into_iter().collect())
        })
        .collect();

    metrics.insert("pipelines", serde_json::Value::Array(pipeline_metrics));

    let metrics_json = serde_json::to_string_pretty(&metrics)?;
    tokio::fs::write(metrics_file, metrics_json).await?;

    tracing::info!("ğŸ“Š Execution metrics exported to: {}", metrics_file);
    println!("ğŸ“Š Metrics exported to: {}", metrics_file);

    Ok(())
}
